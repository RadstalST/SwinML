{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the basic data science lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# import display module\n",
    "from IPython.display import display\n",
    "#import stratify kfold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# import svc,knn    \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "# nlp \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "import re\n",
    "import gensim\n",
    "# https://github.com/alexandres/lexvec#pre-trained-vectors\n",
    "# parameter tuning\n",
    "import optuna\n",
    "#import typing\n",
    "from typing import List, Dict, Tuple, Set, Union, Optional, Callable, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads the data from the csv file\n",
    "x_train = pd.read_csv(\"./Dataset/x_train.csv\",header=None,names=['website','text'])\n",
    "y_train = pd.read_csv(\"./Dataset/y_train.csv\",header=None,names=['positive'])\n",
    "x_test = pd.read_csv(\"./Dataset/x_test.csv\",header=None,names=['website',\"text\"])\n",
    "y_test = pd.read_csv(\"./Dataset/y_test.csv\",header=None,names=['positive'])\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "display(x_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a text pre processing class\n",
    "\n",
    "\n",
    "class TextPreprocessing:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.bag_of_words = []\n",
    "        self.word_counts = {}\n",
    "        pass\n",
    "    def text_cleaning(self,text:str):\n",
    "\n",
    "        re_s = [\n",
    "            #remove the html tags\n",
    "            (r'<.*?>',''),\n",
    "            #remove the urls\n",
    "            (r'http\\S+|www.\\S+',''),\n",
    "            #remove the emails\n",
    "            (r'\\S+@\\S+',''),\n",
    "            #remove the new line\n",
    "            (r'\\n',''),\n",
    "            #remove the special characters\n",
    "            (r'[^\\w\\s]',''),\n",
    "            #remove the numbers\n",
    "            (r'\\d+',''),\n",
    "            #remove the stop words\n",
    "            (r'\\b\\w{1,2}\\b',''),\n",
    "            #remove the extra spaces\n",
    "            (r'\\s+',' ')\n",
    "        ] \n",
    "\n",
    "        for regex in re_s:\n",
    "            text = re.sub(regex[0],regex[1],text)\n",
    "        #convert the text to lower case\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    def text_stemming(self,text:str):\n",
    "        #create the stemmer object\n",
    "        stemmer = self.stemmer\n",
    "        #stem the text\n",
    "        text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    def text_lemmatization(self,text:str):\n",
    "        #create the lemmatizer object\n",
    "        lemmatizer = self.lemmatizer\n",
    "        text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        return text\n",
    "\n",
    "    def text_tokenization(self,text:str):\n",
    "        #tokenize the text\n",
    "        text = text.split()\n",
    "        return text\n",
    "    def text_bag_of_words(self,text:str):\n",
    "        #create the bag of words\n",
    "        token = self.text_tokenization(text)\n",
    "        self.bag_of_words.extend(token)\n",
    "        self.bag_of_words = list(set(self.bag_of_words))\n",
    "        self.text_word_counts(token)\n",
    "        return self.bag_of_words\n",
    "    def text_word_counts(self,token:List[str]):\n",
    "        #create the word counts\n",
    "        # text = self.text_tokenization(text)\n",
    "        for word in token:\n",
    "            if word in self.word_counts:\n",
    "                self.word_counts[word] += 1\n",
    "            else:\n",
    "                self.word_counts[word] = 1\n",
    "        return self.word_counts\n",
    "    def __call__(self,text:str,bow:bool=True):\n",
    "        text = self.text_cleaning(text)\n",
    "        text = self.text_lemmatization(text)\n",
    "        if bow:\n",
    "            self.text_bag_of_words(text)\n",
    "\n",
    "        return text\n",
    "    def get_bag_of_words(self):\n",
    "        return self.bag_of_words,self.word_counts\n",
    "# define stratify k fold\n",
    "def stratified_kfold_cross_validation(x_train,y_train,clf,n_splits=3,):\n",
    "    skf = StratifiedKFold(n_splits=n_splits,random_state=42,shuffle=True)\n",
    "    evals = []\n",
    "    for train_index, test_index in skf.split(x_train[\"vec\"],y_train[\"positive\"]):\n",
    "        _x_train,_x_val = x_train[\"vec\"].iloc[train_index],x_train[\"vec\"].iloc[test_index]\n",
    "        _y_train,_y_val = y_train[\"positive\"].iloc[train_index],y_train[\"positive\"].iloc[test_index]\n",
    "        clf.fit(_x_train,_y_train)\n",
    "        y_pred = clf.predict(_x_val)\n",
    "        y_true = _y_val\n",
    "        evals.append({\n",
    "            \"accuracy\":accuracy_score(y_true,y_pred),\n",
    "            \"f1_score\":f1_score(y_true,y_pred),\n",
    "            \"precision\":precision_score(y_true,y_pred),\n",
    "            \"recall\":recall_score(y_true,y_pred)\n",
    "        })\n",
    "    return evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_preprocess = TextPreprocessing()\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./models/lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define pipeliens for the text preprocessing\n",
    "def vectorize_text(text):\n",
    "    #word2vec\n",
    "    vector = np.zeros(300)\n",
    "    for word in text.split():\n",
    "        if word in model:\n",
    "            vector += model[word]\n",
    "    return vector\n",
    "def text_preprocessing_pipeline(df,t_preprocess:TextPreprocessing,bow=True):\n",
    "    df['processed'] = df['text'].apply(lambda x: t_preprocess(x,bow=bow))\n",
    "\n",
    "    \n",
    "    keys = [(\"vec\",\"vec\"+str(i)) for i in range(300)]\n",
    "    vec_df = pd.DataFrame(columns=keys)\n",
    "    df = df.join(vec_df)\n",
    "    df.columns=pd.MultiIndex.from_tuples([('website',\"value\"),('text','value'),('processed','value')]+keys)\n",
    "    df[keys] = pd.DataFrame(df[('processed','value')].apply(lambda x: vectorize_text(x)).tolist(), index= df.index)\n",
    "    #create multi index so vector columns can be accessed easily\n",
    "    return df\n",
    "\n",
    "x_train = text_preprocessing_pipeline(x_train,t_preprocess)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow, counts = t_preprocess.get_bag_of_words()\n",
    "print(\"bag of words counts\",len(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to train on\n",
    "models = [\n",
    "    (\"Random Forest\",RandomForestClassifier(n_estimators=100,random_state=42)),\n",
    "    (\"Logistic Regression\",LogisticRegression(random_state=42,solver='lbfgs', max_iter=1000)),\n",
    "    (\"SVM\",SVC(random_state=42)),\n",
    "    (\"KNN\",KNeighborsClassifier()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do stratified k fold cross validation on the models\n",
    "evals = []\n",
    "for name,clf in models:\n",
    "    _eval = stratified_kfold_cross_validation(x_train,y_train,clf,n_splits=3)\n",
    "    _eval = pd.DataFrame(_eval)\n",
    "    _eval[\"modelname\"] = name\n",
    "    evals.append(_eval)\n",
    "#concat the evals\n",
    "eval_df = pd.concat(evals)\n",
    "results_df = eval_df.groupby(\"modelname\").mean()\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best results\n",
    "results_df.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter tuning for SVM using optuna\n",
    "def objective(trial):\n",
    "    #define the parameters to tune\n",
    "    params = {\n",
    "        \"C\":trial.suggest_float(\"C\",1e-10,1e10),\n",
    "        \"kernel\":trial.suggest_categorical(\"kernel\",[\"linear\",\"rbf\"]),\n",
    "        \"gamma\":trial.suggest_categorical(\"gamma\",[\"scale\",\"auto\"]),\n",
    "        \"degree\":trial.suggest_int(\"degree\",1,5),\n",
    "        \"coef0\":trial.suggest_float(\"coef0\",1e-10,1e10)\n",
    "    }\n",
    "    #define the model\n",
    "    clf = SVC(**params,random_state=42)\n",
    "    #do stratified k fold cross validation\n",
    "    evals = stratified_kfold_cross_validation(x_train,y_train,clf,n_splits=3)\n",
    "    evals = pd.DataFrame(evals)\n",
    "    #get the mean of the evals\n",
    "    return evals[\"accuracy\"].mean()\n",
    "#optimize the model\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=50,#300 seconds or 50 trials whichever comes first\n",
    "    timeout=300,#300 seconds or 50 trials whichever comes first\n",
    "    # n_jobs=-1,# use all core\n",
    "    gc_after_trial=True, #garbage collect after each trial (free up memory)\n",
    "    show_progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the best model on the whole dataset\n",
    "clf = SVC(random_state=42)\n",
    "clf.fit(x_train[\"vec\"],y_train[\"positive\"])\n",
    "\n",
    "#test the model on the test set\n",
    "_x_test = text_preprocessing_pipeline(x_test,t_preprocess,bow=False)\n",
    "y_pred = clf.predict(_x_test[\"vec\"])\n",
    "y_true = y_test[\"positive\"]\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "f1_score = f1_score(y_true,y_pred)\n",
    "precision = precision_score(y_true,y_pred)\n",
    "recall = recall_score(y_true,y_pred)\n",
    "confusion_mat = confusion_matrix(y_true,y_pred)\n",
    "print(\"accuracy\",accuracy)\n",
    "print(\"f1_score\",f1_score)\n",
    "print(\"precision\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"confusion matrix\",confusion_mat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('metaltf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0bb8f953e5a51e575a0614eea2cfd8668a4922a5c3f594783b10510e33be4fa5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
